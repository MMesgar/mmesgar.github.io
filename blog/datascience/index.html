<!DOCTYPE html>
<!DOCTYPE html>
<html lang="en">
<head>


    
        <!-- Global site tag (gtag.js)  -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169190162-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-169190162-1');
        </script>
    


    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">


    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!-- CSS -->

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />

     <link rel="stylesheet" href="/static/gen/packed.css?cfb8b8c2"> 






    <!-- JS -->








    <!-- Title of the page will be here-->
    <title> Data Science </title>

</head>
<body>

<!-- navigation bar will be here -->
<!-- navbar -->
<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <a class="navbar-brand" href="#">Mohsen Mesgar</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <!-- navigation links will be here -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
        <li class="nav-item active">
            <a class="nav-link" href=" /">Home <span class="sr-only">(current)</span></a>
        </li>





        <li class="nav-item">
            <a class="nav-link" href="/blog/" >Blog</a>
        </li>

        <li class="nav-item">
            <a class="nav-link" href="/certificates/" >Certificate</a>
        </li>





        <li class="nav-item">
            <a class="nav-link" href=" /contact/">Contact</a>
        </li>
    </ul>

    <!-- search form will be here -->





    <!-- social media icons will be here -->

    <a href="https://twitter.com/MohsenMes" class="nav-item nav-link", target="_blank"><i class="fa fa-twitter"></i></a>

     <a href="https://scholar.google.com/citations?user=vKwpx9gAAAAJ&hl=en" class="nav-item nav-link", target="_blank"><i class="fa fa-graduation-cap"></i></a>

    <a href="https://www.linkedin.com/in/mohsen-mesgar/" class="nav-item nav-link", target="_blank"><i class="fa fa-linkedin"></i></a>


</div>
</nav>
<!-- /navbar -->

<!-- container bar will be here -->
<div class="container mt-5 min-height:100%">
    
<h1>
    Data Science
</h1>
    <p></p>
    <p></p>
    <p></p>
<p> Data science is becoming popular.
    In this page, I provide some information about data science.
    I also refer to tools and resources which may aim data scientists.
</p>

  <dl class="row">
    <dt class="col-sm-3">Data science and scientist</dt>
    <dd class="col-sm-9">
        <p>Data science is the study of large quantities of data, which can reveal insights that help organizations make strategic choices.</p>
        <p>Data science is what a data scientist does. </p>
        <p>An aspiring data scientist  should be curious, extremely argumentative and judgmental. Curiosity is absolute must.</p>
    </dd>

    <dt class="col-sm-3">Jupyter Notebooks</dt>
    <dd class="col-sm-9">
        <p>One of the best sources of available interesting jupyter notebooks  is this <span class="c1 c9"><a class="c3" href="https://github.com/jupyter/jupyter/wiki" target="_blank">wiki</a></span>.</p>
        <p>If you have access only to  a link to the jupyter file, you can just grab the URL to that file and past it to the <span class="c1 c9"><a class="c3" href="https://nbviewer.jupyter.org/" target="_blank">NB-Viewer </a></span>.</p>
    </dd>

      <dt class="col-sm-3">Exploratory Data Analysis (EDA)</dt>
      <dd class="col-sm-9">
       <p> EDA is understanding the data sets by summarizing their main characteristics often plotting them visually.</p>
       <p> Through the process of EDA, we can ask to define the problem statement or definition on our data set which is very important.</p>
          <p>Tutorial on general steps in EDA: <span class="c1 c9"><a class="c3" href="https://nbviewer.jupyter.org/github/Tanu-N-Prabhu/Python/blob/master/Exploratory_data_Analysis.ipynb" target="_blank">notebook </a></span>.</p>
      </dd>

      <dt class="col-sm-3">R Programming Language</dt>
      <dd class="col-sm-9">
          <p>R is a statistical programing language used for data processing, data analysis, and machine learning,
          by academics, healthcare and government. </p>
          <p>R is a great tool for visualization.  </p>
          <p> An IDE for R is RStudio.  </p>
          <p> Poplular packages in R are:
              dplyr (data manipulation),
              stringr (string manipulation),
              ggplot (data visualization),
              plotly (web-based data visualization),
              leaflet (interactive data visualization)
              caret (machine learning),
          </p>
      </dd>

      <dt class="col-sm-3">Git</dt>
      <dd class="col-sm-9">
          <p> Git is a distributed version control. It's free and open source. </p>
          <p> GitHub and GitLab are most popular webhost for gir repositories. </p>
          <p> SSH Protocol: a method for secure login from one computer to another. </p>
          <p> Repository is the folders of your project that are set up for version control.</p>
          <p> Fork is a copy of a repository and used to contribute to someone's code or begin your idea from someone's repository.  </p>
          <p> Branch is a snapshot of a repository. The original repo is under the "main" or "master" branch.
          Never push any code that is not tested into the master branch. Instead, create a branch and apply your changes and then test your code.
          If everything is fine then you can merge the your child branch with the main branch. </p>
          <p> Pull request is the process you use to request that someone reviews and approves your changes before they become final </p>
          <p>
              Working directory is a directory on your file system including its files and subdirectories that is associated with git repository.
          </p>
          <p>Tutorial on general git commands: <span class="c1 c9"><a class="c3" href="https://try.github.io/" target="_blank">notebook </a></span>.</p>
      </dd>

      <dt class="col-sm-3">XGBoost</dt>
      <dd class="col-sm-9">
          <p>  It is a gradient-boosted ensemble of decision trees.
              The algorithm was discovered relatively recently and has been used in many solutions and winning data science competitions.</p>
          <p>
      </dd>
      <dt class="col-sm-3">Model Deployment</dt>
      <dd class="col-sm-9">
          <p> ONNX stands for Open Neural Network eXchange, which is format for deploying neural network models.
              Some workflow solutions: Sage Maker by Amazon, Kubeflow by Google (open source), Airflow by Apache, MLFlow by Databricks.
          </p>
          <p>
      </dd>
      <dt class="col-sm-3"> ML Pipeline</dt>
      <dd class="col-sm-9">
          <p> A sequence of step ML researcher performs on daily basis. Many companies try to automtize each of these steps.
              These steps are: (1) Data pre-processing: converting, dealing with missing values, encoding, ...
              (2) Model selection: find top-k estimators, (3) HPO: on selected estimators, (4) feature engineering: find best data transformation sequences, (5) HPO: on estimators after feature engineering.
          </p>
          <p>
      </dd>
      <dt class="col-sm-3"> Data</dt>
      <dd class="col-sm-9">
          <p> When that the data collection stage is complete, data scientists typically use descriptive statistics and visualization techniques to better understand the data and get acquainted with it. Data scientists, essentially, explore the data to:
              (1)understand its content,
              (2)assess its quality,
              (3)discover any interesting preliminary insights, and,
              (4)determine whether additional data is necessary to fill any gaps in the data.
          </p>
          <p>
      </dd>

      <dt class="col-sm-3"> Data Pre-processing (Wrangling) </dt>
      <dd class="col-sm-9">
          <p> (1) Identify and handleing missing values (NaN or np.nan in python). A missing value is captured usually as "?", "N/A", "0" or just a blank cell.
          How to deal with them? There different possibilities such as
              (1.1) Check with the data collection source to know what the actual value should be.
              (1.2) Remove the samples in which one attribute has a missing value.
              (1.3) Remove the variable from the dataset. Choose this option if the size of the data is big.
              (1.4) Replace the missing value by the average of that attribute over other data samples.
              (1.5) Replace the missing value by the most frequent value of the attribute if the sample is categorical and on which you cannot compute the average.
              (1.6) Leave the missing value as missing value.
          </p>
          <p> (2) Data formatting: Data should be consistent and easily understandable.
              Before cleaning the data, think if this unclean data contain some information for your task. If not, then you can clean the data.
              To do so, you can check for
              (2.1) writing style (uppercase, lowercase, etc),
              (2.2) data time format,
              (2.3) abbreviation (NY, New York, etc), and
              (2.4) the data types of attributes and recast them if being not meaningful.
          </p>
          <p> (3) Data normalization (centering/scaling): It helps to make a fair comparison between variables and also an efficient computation.
              The problem is that attributes (features) with large-scaled values (Salary vs Age) affect predictions while not being more important than other attributes.
              How to normalize attribute values?
              (3.1) Max-scaling: x_new = x_old / x_max, where 0< x_new < +1
              (3.2) Min-Max: x_new = (x_old - x_min)/(x_max - x_min),  where 0< x_new < +1, and
              (3.3) Z-score: x_new = (x_old - x_mean)/ x_std, where -1< x_new < +1.
          </p>
          <p> (4) Data binning: The idea is to grop values into bins. It converts numerical values into categorical values.  Binning can improve the accuracy of prediction models.
          Binning also help to have a better understanding of the data distribution. In python, we use linspace and cut function in pandas.
          </p>
          <p> (5) Turning categorical values to numerical variables: The solution is 1-hot encoding. To do so, add dummy variables for each unique category and assign 0 and 1 in each category.
                In python, we can use pandas.get_dummies(df['x']) to convert attribute x to numerical variables.
          </p>
      </dd>


      <dt class="col-sm-3"> Exploratory Data Analysis (EDA) </dt>
      <dd class="col-sm-9">
          <p>
              The first step in data analysis is EDA.
          </p>
          <p> The main goal of EDA is to summarize main characteristics of data,  gain a better understanding of data, uncover relationships between attributes (features or variables), and extract important variables.
              The main question in EDA is that what are the characteristics which have the most impact on the value of target attribute.
          </p>
          <p>
              EDA should be conducted on the whole dataset (including training, validation, and test).
          </p>
          <p>
          Descriptive statistics: provide mean, min, max and std of values of an attribute (variable) in a dataset.
          How to get them? In python, we use (1) pandas.dataframe.describe(include="all"),   (2) for categorical variables we use df['x'].value_counts().to_frame(),
          (3) Boxplots are useful charts for EDA. They help to see the distribution of a variable and also to find the outliers easily.
          The Package ''seaborn'' has the function ''boxplot'' with which you can easily draw a boxplot between a categorical variable (x) and continues variable (y).
           and (4) Scatter plots are used to find relation between two continues variables. Usually, target variable is on y axis and independent variable is on x axis.
          To draw scatter plot, the package "matplotlib" (e.g. "plt.scatter(x,y)") is very handy.
          </p>
          <p>
              Grouping: This technique is applied to categorical variables using "pandas.dataframe.groupby()". The idea is to group the data into subsets according to the categories of a one variable.
              To visualize the results of the groupby() function, pivot tables are very handy. In pivot table, one independent variable is x-axis, the other independent variable is on y-axis and the target variable is the value of the cells.
              The other way to visualize grouping is heatmap. Heatmap is a great way to draw the target variable over multiple variables.
              The package "matplotlib" has function "pcolor(d_pivot, cmap="RdBu")" to draw a heatmap.
          </p>
          <p>
              Correlation: The package "seaborn" has the function "regplot" to draw correlation line between two variables. Pearson correlation is common way to see to what degree are interdependent.
              To compute the Pearson correlation, one can use "scistat" package. It has a function named "stat.pearsonr(df['x'],df['y'])".
              Correlation coefficients can also be visualized by heatmap. The heatmap shows the correlation coefficient between evey two variables in the dataset.
          </p>
          <p>
            Chi-Square: The correlation coefficient works on two continues variables. What should find the interdependence between categorical variables?
              This is what we call "association". The Chi-Square tests a null hypothesis that variables are independent. We try to reject this hypothesis, i.e., we like to have target label be dependent on an attribute variable.
              Please note that Chi-Square can also be used to test if predictions of a model and the ground truth labels are dependent.
              In python, we use "scipy.stats.chi2_contigency(cont_table, correction=True)". It gives us a p_value. If p_values < 0.05 then we reject the null hypothesis.
          </p>
          <p>
              The output of the EDA step should be a subset of features to be used as "predictors" in our model development.
              These predictors are known as independent variables which help to estimate the values of a target variable or dependent variables.
          </p>
      </dd>

      <dt class="col-sm-3">Model Development</dt>
      <dd class="col-sm-9">

          <p>
              Regression: When the target variable is a continues value, we use regression models.
              (1) LinearRegression:
              (1.1) Simple Linear Regression (SLR): y = b_0 + b_1 * x, b_0 is the intercept and b_1 is the coeficient. Note that this function is defined using only one predictor.
              You can easily get SLR from sklearn.linear_models.LinearRegression, or regplot. You can get the intercept and coeficient from the model in sklearn.
              (1.2) Multiple Linear Regression (MLR): y = b_0 + b_1* x_1 + b_2*x_2 + ... + b_n*x_n. This model can be implemented also using sklearn.linear_models.LinearRegression.
              (1.3) Polynomial Regression (PR): y= b_0 + b_1 *x_1 +b_2* (x_2)^2 + ....
              As we see the estimator is still a linear function but over ploynomial features.
              There is a degree for ploynomiality, i.e. 2 in the above equation. This degree is a hyper-parameter that determins the number of final feauteres and consequently the number of parameters (coefficients).
              This degree should be given to the constructor, that is again "sklearn.linear_models.LinearRegression".
              The only difference from MLR is that features should be transformed to the ploynomial space.
              sklearn has a funtion for it, i.e. "z = sklearn.preprocessing.PolynomialFeatures(degree=2).fit_transform(df['x1','x2'])"
              "z" contains the ploynomial features over x1 and x2 from the EDA. For 4 features we get 15 polynomial features.
              We can give "z" to the "model= LinearRegression() model.fit(z)".
              An important point in PR is that in order to combine polynomial combinations of features, the feature values should in the same scale.
              So before running polynomialization over predictors, we should scale their values "sklearn.preprocessing.StandardScaler()".
              If we use "pipe = sklearn.pipeline.Pipeline([StandardScaler(),PolynomialFeatures(degree=2,include_bias=False),LinearRegression()])" then this pipe can be fit directly to the raw features df['x1','x2'].
              (1.4) Ridge: The constructor is in "sklearn.linear_model.Ridge()".
              Ridge regression is a linear regression that is employed in a Multiple regression model when "Multicollinearity" occurs.
              Multicollinearity is when there is a strong relationship among the independent variables.
              Ridge regression is very common with polynomial regression to generalize and to reduce overfitting.
              Important hyperparameters in Ridge are "alpha" and "normalization".
          </p>
      </dd>

      <dt class="col-sm-3">Model Evaluation and Refinement</dt>
      <dd class="col-sm-9">
          <p>
              For evaluating a model always use visualization and measures.
              The term "In-Sample Evaluation" refers to evaluating the model on a training set.
              In contrast, the term "Out-of-Sample Evaluation" refers to evaluating the model on a test set.
              The performance of the model in "Out-of-Sample Evaluation" should approximate the performance of the model in real world.
              When you build a model on the training set and report the perfoamcen on the test set. You should combine train and test sets to train the model on whole data.
              This final model should be used in the deployment time.
          </p>
          <p>
              How to split the datasets? "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.3, random_state=0)".
          </p>
          <p>
              Generalization error: how well our model predicts unseen samples. The error on test set is a proxy for this error.
              Cross validation is an out-of-sample evaluation technique used to find a precise approximation for the generalization error.
              To apply cross validation, one can use the function "sklearn.model_selection.cross_val_score(model, x_data, y_data, cv=k)" where cv is the number of folds.
              The output will be a numpy array of scores on the test set of each fold. Usually we report the mean of the scores.
              To get the predictions, we can use "yhat = sklearn.model_selection.cross_val_predict(model,x_data,y_data, cv=k)".
              The output is a list of predictions for test samples in each fold.
          </p>
          <P>
            Underfitting means that the model is too simple (low number of parameters) to fit to a training set.
            This phenomenon is known as "high bias".
              The bias error can be seen as the error on the training set.
              A model with "high bias" fits loosely to the training data and oversimplifies the model.
              A model with "high bias" always leads to high error on the training and test data.
              Overfitting means that the model is too large (high number of parameters) to generalize to unseen samples.
              When a model overfits, we say the model has "high variance".
              Variance error can be seen as the error on the test set.
              A model with high variance fits too much to training data and does not generalize on the test data.
              As a result, such models perform very well on training data but has high error rates on test data.
              If a model is too simple and has very few parameters then it may have "high bias and low variance".
              On the other hand if our model has large number of parameters then it’s going to have "high variance and low bias".
              Model selection process is about finding the "bias-variance tradeoff".
              By increasing the number of parameters usually the error on the training set decreases.
              The generalization error also decreases until a point on which the generalization error starts to increase.
              Anything on the left side of this point represents underfitting.
              Anyhing on the right side of this point represents overfitting.
              This point, on which the generalization error is minimum, represents the best number of parameters for the model.
              Of course, we have still a bit of error in this point. This error is because of random noise that we may have in the data.
              The error that comes from the noise is known as "irreducible error".
              If the error of the best model (selected at the best point above) is too large then the model is not good enough for fitting to the data.
          </P>
          <P>
              Grid Search: To define the best value for hyperparamrters we can use grid search over predefined values for each parameter.
              The main idea behind the grid search is that the model is evaluated on the "validation set".
              The best combination of hyperparameters are those that minimize the error on the validation set.
              Grid search CV (cross validation) takes a model, a scoring function, the number of folds, and predefined values for hyperparameters.
              The output would be the score of the model for each combination of hyperparameter values.
              The constructor of grid search is "sklearn.model_selection.GridSearchCV(model, parameters, cv=k)".
              parameters is a dictionary whose keys are the name of hyperparameter  used for the model definition and the values are list of hyperparameter values.
              The best model is in "best_model = grid.best_estimator_".
          </P>
          <p>
              Residual Plot: A good way to visualize the variance of the data is to use a residual plot.
              What is a residual?
              The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.
              So what is a residual plot?
              A residual plot is a graph that shows the residuals on the vertical y-axis and the independent variable on the horizontal x-axis.
              What do we pay attention to when looking at a residual plot?
              We look at the spread of the residuals:
              - If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data.
              Why is that? Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.
          </p>
          <p>
              Regression: For Simple linear regression (SLR) models, we use visualization using regression ("seaborn.regplot(x,y)") and residual plot ("seaborn.residplot(x,y)").
              In regression plot we check the correlation and the spread of samples around predicted lines.
              The more spread samples indicate more variance in results and weaker models.
              If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data.
              Why is that? Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.
              For Multiple Linear Regression (MLP) one way to look at the fit of the model is by looking at the distribution plotOne way to look at the fit of the model is by looking at the distribution plot. We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values (ax1=seaborn.distplot(y, hist=False, color="r", label="Actual Value") sns.distplot(Y_hat, hist=False, color="b", label="Fitted Values" , ax=ax1)).
              We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.
              When evaluating our models, not only do we want to visualize the results, but we also want a quantitative measure to determine how accurate the model is.
              Two very important measures that are often used in Statistics to determine the accuracy of a model are:
              (1) R^2 / R-squared ("LinearModel.score(x,y) in sklearn" and "sklearn.metrics.r2_score(y, yhat)" for the Polynomial estimators,
              and (2) Mean Squared Error (MSE)("sklearn.metrics.mean_squared_error(x,y)").
              R-squared: also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.
              The value of the R-squared is the percentage of variation of the response variable (y) that is explained by a linear model. More formally, R^2 = 1- (MSE of the predictions)/(MSE of the average estimator).
              A negative R^2 is a sign of overfitting.
              Mean Squared Error (MSE):The Mean Squared Error measures the average of the squares of errors. That is, the difference between actual value (y) and the estimated value (ŷ).
          </p>
          <p>
          </p>
      </dd>



<!--    <dt class="col-sm-3"> </dt>-->
<!--    <dd class="col-sm-9">-->
<!--      <p>This beloved book by E. B. White, author of Stuart Little and The Trumpet of the Swan, is a classic of children's literature that is "just about perfect."</p>-->
<!--      <p>Charlotte's spiderweb tells of her feelings for a little pig named Wilbur, who simply wants a friend. They also express the love of a girl named Fern, who saved Wilbur's life when he was born the runt of his litter.</p>-->
<!--    </dd>-->


  </dl>






</div>
<!-- /container -->



<!-- Footer -->
<footer class="footer">
      <div class="container  text-center">
        <span class="float-md-left text-muted">@ 2022 Mohsen Mesgar</span>

          <span class="float-md-right text-muted">Modified: Jan-2022</span>

          






      </div>
</footer>
<!-- Footer -->








<script>
$("#search-btn").click(function () {
    $("p").slideToggle("slow");
});
</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.0.0/jquery.min.js"></script>
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>





<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
    window.cookieconsent.initialise({
    "palette": {
        "popup": {
        "background": "#d7f0f4"
        },
        "button": {
        "background": "#39b3c6",
        "text": "#ffffff"
        }
    },
    "theme": "classic"
    });
</script>


</body>
</html>